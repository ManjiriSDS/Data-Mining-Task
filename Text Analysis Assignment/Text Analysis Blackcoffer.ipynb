{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d09abc1f-f259-4d8f-9643-485fcb043819",
   "metadata": {},
   "source": [
    "# Data Extraction & Text Analysis - Blackcoffer Data Science Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9726651-2a62-433a-978c-3601ed730da7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Submitted by Manjiri H. Sawant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3624f6-2529-4d73-aa1f-c600255e23e6",
   "metadata": {},
   "source": [
    "#### **Objectives:**\n",
    "\n",
    "The objective of this assignment is to extract textual data articles from the given URL and perform text analysis to compute variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59865808-3f53-415f-a92d-060f66986ebe",
   "metadata": {},
   "source": [
    "1. `Data Extraction`\n",
    "2. `Data Analysis`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a69134-597a-43a4-8c13-73c7b5bcdde8",
   "metadata": {},
   "source": [
    "`Tool Used`: **Python Jupyter Lab**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc9546-4acf-4317-9a48-72b5325d92e9",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f4752f-f2ff-4091-ab81-f2adc7d078a7",
   "metadata": {},
   "source": [
    "1. `BeautifulSoup` - Format and Scrap the data from the HTML\n",
    "2. `Selenium` - Controlling web browser through programs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b052bf-e6b0-4e45-b3f4-7ed00b4fd6ba",
   "metadata": {},
   "source": [
    "**Steps**\n",
    "\n",
    "1. Identify URL\n",
    "2. Inspect HTML code\n",
    "3. Find the HTML tag for the element that you want to extract.\n",
    "4. Write some code to scrap this data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7171c031-ca87-4137-b01a-30f77e790414",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc53dd-fd61-4c6c-ae02-573a909b30c8",
   "metadata": {},
   "source": [
    "`Look for these variables in the analysis:`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0cf2dc-6dd5-443d-9d34-bb25810b85b3",
   "metadata": {},
   "source": [
    "1.\tPOSITIVE SCORE\n",
    "2.\tNEGATIVE SCORE\n",
    "3.\tPOLARITY SCORE\n",
    "4.\tSUBJECTIVITY SCORE\n",
    "5.\tAVG SENTENCE LENGTH\n",
    "6.\tPERCENTAGE OF COMPLEX WORDS\n",
    "7.\tFOG INDEX\n",
    "8.\tAVG NUMBER OF WORDS PER SENTENCE\n",
    "9.\tCOMPLEX WORD COUNT\n",
    "10.\tWORD COUNT\n",
    "11.\tSYLLABLE PER WORD\n",
    "12.\tPERSONAL PRONOUNS\n",
    "13.\tAVG WORD LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420bb59-23f6-4145-8fe6-588fa2f2774e",
   "metadata": {},
   "source": [
    "#### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f30971-6bd9-4d84-b14d-989ba00b8f6f",
   "metadata": {},
   "source": [
    "##### The following code written in Python 3.x. Libraries provide pre-written functionally to perform necessary tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da96fe7-8a6b-4a10-9013-f2bd97fa19a4",
   "metadata": {},
   "source": [
    "## 1. Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db4d85-d765-4ccc-9d45-7cda343ec4e2",
   "metadata": {},
   "source": [
    "* For each of the articles, given in the `input.xlsx` file, extract the article text and save the extracted article in a text file with URL_ID as its file name.\n",
    "* While extracting text, please make sure your program extracts only the article title and the article text. It should not extract the website header, footer, or anything other than the article text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f704d12-0dc3-4001-a35c-cba6917e3ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported Required Libraries\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptionsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1297142c-2d73-4677-b43d-758f00e2ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "\n",
    "df = pd.read_excel(\"Input.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d288a5f4-cd6c-49ba-9662-ad4bd280914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here 1st Column will be extracted ie 'URL_ID'\n",
    "# Converting specific df columns\n",
    "\n",
    "u_id = df[\"URL_ID\"].astype(int).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba1aa135-1030-4f53-b8e0-e54345edbdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here 2nd Column will be extracted ie 'URL'\n",
    "# Converting specific df columns\n",
    "\n",
    "u_l = df[\"URL\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6b598ee-3bea-422e-b360-2fece51ad855",
   "metadata": {},
   "outputs": [],
   "source": [
    "completed = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231a7a18-5ceb-4afe-a26e-d20d3ec8b00d",
   "metadata": {},
   "source": [
    "Identify below features based on them and try to scrape out the relevant data from website :\n",
    "* article title ='?'\n",
    "* article text = '?'\n",
    "* Article Title = **`h1`** `class - 'entry-title'`\n",
    "* Article Text = **`div`** `class  - 'td-post-content'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fe465c-5a79-42cb-8baa-5f4b039f41ae",
   "metadata": {},
   "source": [
    "**Tags are very important while scraping data from particular website.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e3e1b7-758a-4028-a0c1-2cbcf0cafa7f",
   "metadata": {},
   "source": [
    "**soup.find()** is used for finding out the first tag with the specified name or id and \n",
    "returning an object of type bs4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c57b56-d8e2-47f0-b8bd-108103a992ff",
   "metadata": {},
   "source": [
    "* Through WebDriver, Selenium supports all major browsers such as Chrome/Chromium, Firefox, Internet Explorer, Edge, Opera, and Safari. \n",
    "* WebDriver drives the browser using the browser’s built-in support for automation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088c2d1-b43c-4284-9304-3e4f5c07476c",
   "metadata": {},
   "source": [
    "* **Gecko driver** links between selenium test and the Firefox browser keep **headless** to access any website easily nothing will appear on screen. \n",
    "* Everything is done on the backend side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d3591a2-d63f-4cf0-bb61-717f5ab99db9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "File written successfully\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# URL as an input \n",
    "# Iterating till the range and export as txt file\n",
    "\n",
    "\n",
    "time_out = time.time() + 60*1 # 1 min from now\n",
    "\n",
    "for ele in u_l:\n",
    "    try:\n",
    "        options = FirefoxOptions()\n",
    "        options.add_argument(\"--headless\")\n",
    "        driver = webdriver.Firefox(options=options)\n",
    "        \n",
    "        \n",
    "        driver.get(ele)\n",
    "        content = driver.page_source\n",
    "    \n",
    "    finally:\n",
    "        try:\n",
    "            driver.close()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "    soup = BeautifulSoup(content, features = \"html.parser\")\n",
    "\n",
    "    dataset = []\n",
    "    \n",
    "    #title\n",
    "    title = soup.find(\"h1\", attrs = {\"class\" : \"entry-title\"})\n",
    "    if title is None:\n",
    "        dataset.append(np.NaN)\n",
    "    else:\n",
    "        dataset.append(title.text)\n",
    "    \n",
    "    \n",
    "    #post\n",
    "    post = soup.find(\"div\", attrs = {\"class\": \"td-post-content\"})\n",
    "    if post is None:\n",
    "        dataset.append(np.NaN)\n",
    "    else:\n",
    "        dataset.append(post.text)\n",
    "\n",
    "\n",
    "    \n",
    "    x = u_id[0]\n",
    "    print(x)\n",
    "    \n",
    "\n",
    "    with open('F:/dataset/{}.txt'.format(x), 'w+', encoding='utf-8') as t:\n",
    "        for items in dataset:\n",
    "            t.write('%s\\n' %items)\n",
    "    t.close()\n",
    "    \n",
    "\n",
    "    completed.append(ele)\n",
    "    \n",
    "#     print(completed)\n",
    "    \n",
    "    print(\"File written successfully\")\n",
    "    \n",
    "    \n",
    "\n",
    "    u_id.pop(0)\n",
    "    \n",
    "    if time.time() > time_out:\n",
    "        break\n",
    "        \n",
    "for ele in completed:\n",
    "    if ele in u_l:\n",
    "        u_l.remove(ele)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5b060-2826-4ec2-89dc-57ff58998491",
   "metadata": {},
   "source": [
    "# 2. Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d3bafe-2eec-4293-a60b-6be1f4710852",
   "metadata": {},
   "source": [
    "Perform text analysis to drive sentimental opinion, sentiment scores, readability, passive words, personal pronouns and etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32131894-0090-40e8-96b6-0ab84fc366d5",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97042a6-c2f4-4314-ae91-28a254a7ccbe",
   "metadata": {},
   "source": [
    "Sentimental analysis is the process of determining whether a piece of writing is positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a009d2e8-735a-4639-b3be-685bb1a9b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d912ffb8-b272-424d-a1f5-b30f900002cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and directories in ' C:/Users/User/dataset/ ' :\n"
     ]
    }
   ],
   "source": [
    "# get the list of all text files\n",
    "\n",
    "path = \"C:/Users/User/dataset/\"\n",
    "\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "print(\"Files and directories in '\", path, \"' :\")\n",
    "\n",
    "# prints all files\n",
    "\n",
    "# print(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4935015f-0db0-40fe-8899-e61ab2a0e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to sort a list of strings\n",
    "# Using sort() function with key as len sort \n",
    "# sort within the given list based on the length of the element present withing\n",
    "\n",
    "dir_list.sort(key = len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fb6d8134-df13-4e0a-903e-fe9df813dd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['37.txt', '38.txt', '39.txt', '40.txt', '41.txt', '42.txt', '43.txt', '44.txt', '45.txt', '46.txt', '47.txt', '48.txt', '49.txt', '50.txt', '51.txt', '52.txt', '53.txt', '54.txt', '55.txt', '56.txt', '57.txt', '58.txt', '59.txt', '60.txt', '61.txt', '62.txt', '63.txt', '64.txt', '65.txt', '66.txt', '67.txt', '68.txt', '69.txt', '70.txt', '71.txt', '72.txt', '73.txt', '74.txt', '75.txt', '76.txt', '77.txt', '78.txt', '79.txt', '80.txt', '81.txt', '82.txt', '83.txt', '84.txt', '85.txt', '86.txt', '87.txt', '88.txt', '89.txt', '90.txt', '91.txt', '92.txt', '93.txt', '94.txt', '95.txt', '96.txt', '97.txt', '98.txt', '99.txt', '100.txt', '101.txt', '102.txt', '103.txt', '104.txt', '105.txt', '106.txt', '107.txt', '108.txt', '109.txt', '110.txt', '111.txt', '112.txt', '113.txt', '114.txt', '115.txt', '116.txt', '117.txt', '118.txt', '119.txt', '120.txt', '121.txt', '122.txt', '123.txt', '124.txt', '125.txt', '126.txt', '127.txt', '128.txt', '129.txt', '130.txt', '131.txt', '132.txt', '133.txt', '134.txt', '135.txt', '136.txt', '137.txt', '138.txt', '139.txt', '140.txt', '141.txt', '142.txt', '143.txt', '144.txt', '145.txt', '146.txt', '147.txt', '148.txt', '149.txt', '150.txt']\n"
     ]
    }
   ],
   "source": [
    "print(dir_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1563e54a-48cb-425b-b596-bc8aab73e51a",
   "metadata": {},
   "source": [
    "**The Master Dictionary Found Here:**\n",
    "* https://sraf.nd.edu/loughranmcdonald-master-dictionary/\n",
    "* https://drive.google.com/file/d/17CmUZM9hGUdGYjCXcjQLyybjTrcjrhik/view"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20defc1e-de10-4bbc-9c93-52ee06cb0cdd",
   "metadata": {},
   "source": [
    "for x in dir_list:\n",
    "    f = open((\"C:/Users/User/dataset/{}\".format(x)), encoding = \"mbcs\").read().lower()\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "354bcba1-5d36-4a35-9fe3-2392b34ef5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pandas Dataframe\n",
    "# change the string to lowercase in Pandas Dataframe using df['column name'].str.lower()\n",
    "# Python zip() method takes iterable or containers and \n",
    "# returns a single iterator object, having mapped values from all the containers. \n",
    "\n",
    "df = pd.read_csv('Loughran-McDonald_MasterDictionary_1993-2021.csv')\n",
    "\n",
    "sydict = dict(zip(df.Word.str.lower(),df.Syllables))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e2c40d3-912d-4d86-b333-fbd963d977af",
   "metadata": {},
   "source": [
    "print(sydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "284d7eee-e80b-4c03-9afd-a2c9d85aa876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(text):\n",
    "    num_of_words = 0\n",
    "    lines = text.split()\n",
    "    for w in lines:\n",
    "        if not w.isnumeric():\n",
    "            num_of_words += 1\n",
    "    return num_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10635a3e-b0e5-4328-8388-73201b893a60",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Word Count "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f58373-c12a-4ed3-beb4-5e84ec17f752",
   "metadata": {},
   "source": [
    "`We count the total cleaned words present in the text by:`\n",
    "1.\tremoving the stop words (using stopwords class of nltk package).\n",
    "2.\tremoving any punctuations like ? ! , . from the word before counting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be6f5e2-4925-40a7-bf87-b8181ffc80c1",
   "metadata": {},
   "source": [
    "**The StopWords list Found Here:**\n",
    "* https://sraf.nd.edu/textual-analysis/stopwords/\n",
    "* https://drive.google.com/file/d/0B4niqV00F3msSktONVhfaElXeEk/view?resourcekey=0-3hFK5VYPXA7R_Q2LvA-SOw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2850a40f-bbdf-4987-ae57-3f8f56d2eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = open('C:/Users/User/Case Study/blackcoffer assignment/files/StopWords_GenericLong.txt').read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ee7abe5f-d8b9-464e-aaa7-4d173ce96c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple python function 1\n",
    "# total cleaned word present in the text\n",
    "\n",
    "\n",
    "def clean_word(text):\n",
    "    filtered = []\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    clean = tokenizer.tokenize(text)\n",
    "    \n",
    "    for w in clean:\n",
    "        if w not in stopwords:\n",
    "            if w.isalpha():\n",
    "                filtered.append(w)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5fd76ff2-2644-4217-b823-1bda6ff13bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple python function 2\n",
    "# count total cleaned word present in the text\n",
    "\n",
    "\n",
    "def word_count(text):\n",
    "    filtered = []\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    clean = tokenizer.tokenize(text)\n",
    "    \n",
    "    for w in clean:\n",
    "        if w not in stopwords:\n",
    "            if w.isalpha():\n",
    "                filtered.append(w)\n",
    "    return len(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ddf858-eb9f-43ba-9a16-c6266c5e504a",
   "metadata": {},
   "source": [
    "## Positive Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f674e-8b07-40bd-8126-5a0ded7dfc36",
   "metadata": {},
   "source": [
    "**This file contains a list of POSITIVE opinion words (or sentiment words).**\n",
    "\n",
    "* This file and the papers can all be downloaded from \n",
    "http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html\n",
    "\n",
    "If you use this list, please cite one of the following two papers:\n",
    "   *  Minqing Hu and Bing Liu. \"Mining and Summarizing Customer Reviews.\" Proceedings of the ACM SIGKDD International\n",
    "       Conference on Knowledge Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, Washington, USA, \n",
    "   *   Bing Liu, Minqing Hu and Junsheng Cheng. \"Opinion Observer: Analyzing and Comparing Opinions on the Web.\" Proceedings of the 14th International World Wide Web conference (WWW-2005), May 10-14, 2005, Chiba, Japan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "de5857fd-9017-4b24-a8b7-78b60505b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positve words\n",
    "# opening the file in read mode\n",
    "file1 = open('E:/Blackcoffer case study/positive-words.txt','r')\n",
    "\n",
    "# reading the file\n",
    "data1 = file1.read()\n",
    "\n",
    "# splitting the text \n",
    "p_word = data1.split()\n",
    "# print(p_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e4cf23b1-810c-4eb8-82e8-cb5c7092378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_score(text):\n",
    "    values = []\n",
    "    for item in text:\n",
    "        if item in p_word:\n",
    "            score = +1\n",
    "            values.append(score)\n",
    "    return sum(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b944f160-4cc3-4600-b971-a1acc68fbae1",
   "metadata": {},
   "source": [
    "## Negative Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140d7882-5788-46fd-b606-1a17ca2bfb83",
   "metadata": {},
   "source": [
    "**Dictionary of Negative Words Found Here :**\n",
    "\n",
    "* This file contains a list of NEGATIVE opinion words (or sentiment words).\n",
    "\n",
    "* This file and the papers can all be downloaded from:\n",
    "    \n",
    "  http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html\n",
    "\n",
    "If you use this list, please cite one of the following two papers:\n",
    "   *  Minqing Hu and Bing Liu. \"Mining and Summarizing Customer Reviews.\" Proceedings of the ACM SIGKDD International\n",
    "       Conference on Knowledge Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, Washington, USA, \n",
    "   *   Bing Liu, Minqing Hu and Junsheng Cheng. \"Opinion Observer: Analyzing and Comparing Opinions on the Web.\" Proceedings of the 14th International World Wide Web conference (WWW-2005), May 10-14, 2005, Chiba, Japan.\n",
    "\n",
    "https://gist.github.com/mkulakowski2/4289441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fa83f27c-1546-436a-9308-a63f84e72f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative words\n",
    "\n",
    "#opening the file in read mode\n",
    "file2 = open('E:/Blackcoffer case study/negative-words.txt','r')\n",
    "\n",
    "#reading the file\n",
    "data2 = file2.read()\n",
    "\n",
    "#spliting the text\n",
    "n_word = data2.split()\n",
    "# print(n_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f613cd49-2360-41c4-858a-574e32118ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_score(text):\n",
    "    values = []\n",
    "    for item in text:\n",
    "        if item in n_word:\n",
    "            score = -1\n",
    "            values.append(score)\n",
    "    final = sum(values) * -1\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eb8b92-b010-43cb-b222-f80c85ca8365",
   "metadata": {},
   "source": [
    "## Polarity Score\n",
    "\n",
    "This is the score that determines if a given text is positive or negative in nature. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d2564-5609-446a-8786-dcb0240ddbad",
   "metadata": {},
   "source": [
    "It is calculated by using the formula: \n",
    "**Polarity Score** = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
    "\n",
    "`Range is from -1 to +1`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d8ac04-1c6b-42c2-850f-489cce4e5dce",
   "metadata": {},
   "source": [
    "## Subjectivity Score\n",
    "\n",
    "This is the score that determines if a given text is objective or subjective. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a904121-d38e-4536-9460-0a02871d70d5",
   "metadata": {},
   "source": [
    "It is calculated by using the formula: \n",
    "\n",
    "**Subjectivity Score** = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
    "\n",
    "`Range is from 0 to +1`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "485d54bd-15a3-4386-a752-656f216c7bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.txt\n",
      "Successful\n",
      "38.txt\n",
      "Successful\n",
      "39.txt\n",
      "Successful\n",
      "40.txt\n",
      "Successful\n",
      "41.txt\n",
      "Successful\n",
      "42.txt\n",
      "Successful\n",
      "43.txt\n",
      "Successful\n",
      "44.txt\n",
      "Successful\n",
      "45.txt\n",
      "Successful\n",
      "46.txt\n",
      "Successful\n",
      "47.txt\n",
      "Successful\n",
      "48.txt\n",
      "Successful\n",
      "49.txt\n",
      "Successful\n",
      "50.txt\n",
      "Successful\n",
      "51.txt\n",
      "Successful\n",
      "52.txt\n",
      "Successful\n",
      "53.txt\n",
      "Successful\n",
      "54.txt\n",
      "Successful\n",
      "55.txt\n",
      "Successful\n",
      "56.txt\n",
      "Successful\n",
      "57.txt\n",
      "Successful\n",
      "58.txt\n",
      "Successful\n",
      "59.txt\n",
      "Successful\n",
      "60.txt\n",
      "Successful\n",
      "61.txt\n",
      "Successful\n",
      "62.txt\n",
      "Successful\n",
      "63.txt\n",
      "Successful\n",
      "64.txt\n",
      "Successful\n",
      "65.txt\n",
      "Successful\n",
      "66.txt\n",
      "Successful\n",
      "67.txt\n",
      "Successful\n",
      "68.txt\n",
      "Successful\n",
      "69.txt\n",
      "Successful\n",
      "70.txt\n",
      "Successful\n",
      "71.txt\n",
      "Successful\n",
      "72.txt\n",
      "Successful\n",
      "73.txt\n",
      "Successful\n",
      "74.txt\n",
      "Successful\n",
      "75.txt\n",
      "Successful\n",
      "76.txt\n",
      "Successful\n",
      "77.txt\n",
      "Successful\n",
      "78.txt\n",
      "Successful\n",
      "79.txt\n",
      "Successful\n",
      "80.txt\n",
      "Successful\n",
      "81.txt\n",
      "Successful\n",
      "82.txt\n",
      "Successful\n",
      "83.txt\n",
      "Successful\n",
      "84.txt\n",
      "Successful\n",
      "85.txt\n",
      "Successful\n",
      "86.txt\n",
      "Successful\n",
      "87.txt\n",
      "Successful\n",
      "88.txt\n",
      "Successful\n",
      "89.txt\n",
      "Successful\n",
      "90.txt\n",
      "Successful\n",
      "91.txt\n",
      "Successful\n",
      "92.txt\n",
      "Successful\n",
      "93.txt\n",
      "Successful\n",
      "94.txt\n",
      "Successful\n",
      "95.txt\n",
      "Successful\n",
      "96.txt\n",
      "Successful\n",
      "97.txt\n",
      "Successful\n",
      "98.txt\n",
      "Successful\n",
      "99.txt\n",
      "Successful\n",
      "100.txt\n",
      "Successful\n",
      "101.txt\n",
      "Successful\n",
      "102.txt\n",
      "Successful\n",
      "103.txt\n",
      "Successful\n",
      "104.txt\n",
      "Successful\n",
      "105.txt\n",
      "Successful\n",
      "106.txt\n",
      "Successful\n",
      "107.txt\n",
      "Successful\n",
      "108.txt\n",
      "Successful\n",
      "109.txt\n",
      "Successful\n",
      "110.txt\n",
      "Successful\n",
      "111.txt\n",
      "Successful\n",
      "112.txt\n",
      "Successful\n",
      "113.txt\n",
      "Successful\n",
      "114.txt\n",
      "Successful\n",
      "115.txt\n",
      "Successful\n",
      "116.txt\n",
      "Successful\n",
      "117.txt\n",
      "Successful\n",
      "118.txt\n",
      "Successful\n",
      "119.txt\n",
      "Successful\n",
      "120.txt\n",
      "Successful\n",
      "121.txt\n",
      "Successful\n",
      "122.txt\n",
      "Successful\n",
      "123.txt\n",
      "Successful\n",
      "124.txt\n",
      "Successful\n",
      "125.txt\n",
      "Successful\n",
      "126.txt\n",
      "Successful\n",
      "127.txt\n",
      "Successful\n",
      "128.txt\n",
      "Successful\n",
      "129.txt\n",
      "Successful\n",
      "130.txt\n",
      "Successful\n",
      "131.txt\n",
      "Successful\n",
      "132.txt\n",
      "Successful\n",
      "133.txt\n",
      "Successful\n",
      "134.txt\n",
      "Successful\n",
      "135.txt\n",
      "Successful\n",
      "136.txt\n",
      "Successful\n",
      "137.txt\n",
      "Successful\n",
      "138.txt\n",
      "Successful\n",
      "139.txt\n",
      "Successful\n",
      "140.txt\n",
      "Successful\n",
      "141.txt\n",
      "Successful\n",
      "142.txt\n",
      "Successful\n",
      "143.txt\n",
      "Successful\n",
      "144.txt\n",
      "Successful\n",
      "145.txt\n",
      "Successful\n",
      "146.txt\n",
      "Successful\n",
      "147.txt\n",
      "Successful\n",
      "148.txt\n",
      "Successful\n",
      "149.txt\n",
      "Successful\n",
      "150.txt\n",
      "Successful\n",
      "CPU times: total: 6.88 s\n",
      "Wall time: 7.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cl_w = []         # total number of clean words\n",
    "score_1 = []      # Positive Score\n",
    "score_2 = []      # Negative Score\n",
    "score_3 = []      # Polarity Score\n",
    "score_4 = []      # Subjectivity Score\n",
    "\n",
    "\n",
    "for x in dir_list:\n",
    "    #absolute path is given\n",
    "    article = open((\"C:/Users/User/dataset/{}\".format(x)), encoding = \"mbcs\").read().lower()\n",
    "    \n",
    "    # Clean Words\n",
    "    f = clean_word(article)\n",
    "   \n",
    "    \n",
    "    # Clean Word Count\n",
    "    m = word_count(article)\n",
    "    cl_w.append(m)\n",
    "   \n",
    "    \n",
    "    # Positive Score\n",
    "    ps = positive_score(f)\n",
    "    score_1.append(ps)\n",
    "   \n",
    "   \n",
    "    # Negative Score\n",
    "    ns = negative_score(f)\n",
    "    score_2.append(ns)\n",
    "   \n",
    "   \n",
    "    # Polarity Score\n",
    "    w = round(((ps-ns)/((ps+ns) + 0.000001)),2)\n",
    "    score_3.append(w)\n",
    "    \n",
    "    # Subjectivity Score\n",
    "    v = round(((ps + ns)/(m + 0.000001)),2)\n",
    "    score_4.append(v)\n",
    "    \n",
    "    \n",
    "    print(x)\n",
    "    print('Successful')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a66ba4b-b117-4ef5-93f3-d255b14e470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cl_w)\n",
    "# print(score_1)\n",
    "# print(score_2)\n",
    "# print(score_3)\n",
    "# print(score_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0647416-57dc-43c2-b03b-a0f9af0dbf72",
   "metadata": {},
   "source": [
    "## Average Number of Words Per Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4141feee-c474-4937-9541-28c1434aa768",
   "metadata": {},
   "source": [
    "`The formula for calculating is:`\n",
    "\n",
    "**Average Number of Words Per Sentence** = the total number of words / the total number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ec3a0b08-e722-4378-b767-ab33cba9870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple python function\n",
    "# calculate average sentence length\n",
    "\n",
    "def avg_sent_len(text):\n",
    "    s = sent_tokenize(text)\n",
    "    ns = len(s)\n",
    "    asl = round((k/ns),2)\n",
    "    return asl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9d894-91fd-4078-8182-3d3c7f51fa19",
   "metadata": {},
   "source": [
    "## Complex Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cffc67-69cc-4dab-a941-1ab12647a758",
   "metadata": {},
   "source": [
    "`Complex words are words in the text that contain more than two syllables.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ec0c8c77-61f3-4043-b99f-a2a28ba29b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple python function 4\n",
    "# count complex word which has more than two syllables\n",
    "\n",
    "def complex_word_count(t_dict,t_lst):\n",
    "    complx = []\n",
    "    for w in t_lst:\n",
    "        for key,val in t_dict.items():\n",
    "            if w == key:\n",
    "                if val > 2:\n",
    "                    complx.append(w)\n",
    "    return(len(complx))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9e08a51-9ddd-452b-bdf2-2fb007d4993d",
   "metadata": {},
   "source": [
    "print(dir_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0547ab25-3209-4563-b602-5c49ee2728d9",
   "metadata": {},
   "source": [
    "## Analysis of Readability\n",
    "\n",
    "Analysis of Readability is calculated using the Gunning Fox index formula described below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df8f33e-cb9a-4c41-98d2-d4f3c9b37d55",
   "metadata": {},
   "source": [
    "* **Average Sentence Length** = the number of words / the number of sentences\n",
    "* **Percentage of Complex words** = the number of complex words / the number of words \n",
    "* **Fog Index** = 0.4 * (Average Sentence Length + Percentage of Complex words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5464cf37-38a9-4720-9ea2-a3c035fd0a29",
   "metadata": {},
   "source": [
    "* **The Gunning Fog** formula generates a grade level between `0 and 20.` \n",
    "* It estimates the education level required to understand the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4f374880-ad76-4ea9-b53a-e586f309b150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.txt\n",
      "Successful\n",
      "38.txt\n",
      "Successful\n",
      "39.txt\n",
      "Successful\n",
      "40.txt\n",
      "Successful\n",
      "41.txt\n",
      "Successful\n",
      "42.txt\n",
      "Successful\n",
      "43.txt\n",
      "Successful\n",
      "44.txt\n",
      "Successful\n",
      "45.txt\n",
      "Successful\n",
      "46.txt\n",
      "Successful\n",
      "47.txt\n",
      "Successful\n",
      "48.txt\n",
      "Successful\n",
      "49.txt\n",
      "Successful\n",
      "50.txt\n",
      "Successful\n",
      "51.txt\n",
      "Successful\n",
      "52.txt\n",
      "Successful\n",
      "53.txt\n",
      "Successful\n",
      "54.txt\n",
      "Successful\n",
      "55.txt\n",
      "Successful\n",
      "56.txt\n",
      "Successful\n",
      "57.txt\n",
      "Successful\n",
      "58.txt\n",
      "Successful\n",
      "59.txt\n",
      "Successful\n",
      "60.txt\n",
      "Successful\n",
      "61.txt\n",
      "Successful\n",
      "62.txt\n",
      "Successful\n",
      "63.txt\n",
      "Successful\n",
      "64.txt\n",
      "Successful\n",
      "65.txt\n",
      "Successful\n",
      "66.txt\n",
      "Successful\n",
      "67.txt\n",
      "Successful\n",
      "68.txt\n",
      "Successful\n",
      "69.txt\n",
      "Successful\n",
      "70.txt\n",
      "Successful\n",
      "71.txt\n",
      "Successful\n",
      "72.txt\n",
      "Successful\n",
      "73.txt\n",
      "Successful\n",
      "74.txt\n",
      "Successful\n",
      "75.txt\n",
      "Successful\n",
      "76.txt\n",
      "Successful\n",
      "77.txt\n",
      "Successful\n",
      "78.txt\n",
      "Successful\n",
      "79.txt\n",
      "Successful\n",
      "80.txt\n",
      "Successful\n",
      "81.txt\n",
      "Successful\n",
      "82.txt\n",
      "Successful\n",
      "83.txt\n",
      "Successful\n",
      "84.txt\n",
      "Successful\n",
      "85.txt\n",
      "Successful\n",
      "86.txt\n",
      "Successful\n",
      "87.txt\n",
      "Successful\n",
      "88.txt\n",
      "Successful\n",
      "89.txt\n",
      "Successful\n",
      "90.txt\n",
      "Successful\n",
      "91.txt\n",
      "Successful\n",
      "92.txt\n",
      "Successful\n",
      "93.txt\n",
      "Successful\n",
      "94.txt\n",
      "Successful\n",
      "95.txt\n",
      "Successful\n",
      "96.txt\n",
      "Successful\n",
      "97.txt\n",
      "Successful\n",
      "98.txt\n",
      "Successful\n",
      "99.txt\n",
      "Successful\n",
      "100.txt\n",
      "Successful\n",
      "101.txt\n",
      "Successful\n",
      "102.txt\n",
      "Successful\n",
      "103.txt\n",
      "Successful\n",
      "104.txt\n",
      "Successful\n",
      "105.txt\n",
      "Successful\n",
      "106.txt\n",
      "Successful\n",
      "107.txt\n",
      "Successful\n",
      "108.txt\n",
      "Successful\n",
      "109.txt\n",
      "Successful\n",
      "110.txt\n",
      "Successful\n",
      "111.txt\n",
      "Successful\n",
      "112.txt\n",
      "Successful\n",
      "113.txt\n",
      "Successful\n",
      "114.txt\n",
      "Successful\n",
      "115.txt\n",
      "Successful\n",
      "116.txt\n",
      "Successful\n",
      "117.txt\n",
      "Successful\n",
      "118.txt\n",
      "Successful\n",
      "119.txt\n",
      "Successful\n",
      "120.txt\n",
      "Successful\n",
      "121.txt\n",
      "Successful\n",
      "122.txt\n",
      "Successful\n",
      "123.txt\n",
      "Successful\n",
      "124.txt\n",
      "Successful\n",
      "125.txt\n",
      "Successful\n",
      "126.txt\n",
      "Successful\n",
      "127.txt\n",
      "Successful\n",
      "128.txt\n",
      "Successful\n",
      "129.txt\n",
      "Successful\n",
      "130.txt\n",
      "Successful\n",
      "131.txt\n",
      "Successful\n",
      "132.txt\n",
      "Successful\n",
      "133.txt\n",
      "Successful\n",
      "134.txt\n",
      "Successful\n",
      "135.txt\n",
      "Successful\n",
      "136.txt\n",
      "Successful\n",
      "137.txt\n",
      "Successful\n",
      "138.txt\n",
      "Successful\n",
      "139.txt\n",
      "Successful\n",
      "140.txt\n",
      "Successful\n",
      "141.txt\n",
      "Successful\n",
      "142.txt\n",
      "Successful\n",
      "143.txt\n",
      "Successful\n",
      "144.txt\n",
      "Successful\n",
      "145.txt\n",
      "Successful\n",
      "146.txt\n",
      "Successful\n",
      "147.txt\n",
      "Successful\n",
      "148.txt\n",
      "Successful\n",
      "149.txt\n",
      "Successful\n",
      "150.txt\n",
      "Successful\n",
      "CPU times: total: 3min 35s\n",
      "Wall time: 3min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num = []    # Total Number of Words\n",
    "hard = []   # Complex Word Count\n",
    "sent = []   # Average Sentence Length\n",
    "percent = []  # Percentage of Complex Words\n",
    "fog_index = [] # Fog Index\n",
    "\n",
    "\n",
    "for x in dir_list:\n",
    "    article = open((\"C:/Users/User/dataset/{}\".format(x)), encoding = \"mbcs\").read().lower()\n",
    "    \n",
    "    # Clean Words\n",
    "    f = clean_word(article)\n",
    "    \n",
    "    # Number of Words\n",
    "    k = count(article)\n",
    "    num.append(k)\n",
    "    \n",
    "    # Complex Word Count\n",
    "    cw = complex_word_count(sydict,f)\n",
    "    hard.append(cw)\n",
    "    \n",
    "    # Average Sentence Length\n",
    "    al = avg_sent_len(article)\n",
    "    sent.append(al)\n",
    "    \n",
    "    # Percentage of Complex Words\n",
    "    pcw = round((cw/k),2)\n",
    "    percent.append(pcw)\n",
    "    \n",
    "    # Fog Index\n",
    "    fi = round((0.4* (al+pcw)),2)\n",
    "    fog_index.append(fi)\n",
    "    \n",
    "    \n",
    "    print(x)\n",
    "    print('Successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "94d838a7-3b25-44b2-9fe1-f40b68cd2eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(hard)\n",
    "# print(sent)\n",
    "# print(fog_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8469bbf-f99a-415a-b346-ab1503f9fe9c",
   "metadata": {},
   "source": [
    "## Average Word Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803ce9da-3294-4a3c-871a-9de064d52e9d",
   "metadata": {},
   "source": [
    "Average Word Length is calculated by the formula:\n",
    "\n",
    "`Sum of the total number of characters in each word/Total number of words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e45812a5-dc55-4cf5-8fa7-0e8061399d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using regular Expression\n",
    "\n",
    "def avg_word_length(text):\n",
    "    tk1 = RegexpTokenizer(\"[\\w']+\") # tokenize word\n",
    "    tk2 = RegexpTokenizer(\"[\\w']\")  # tokenize char in each word\n",
    "    \n",
    "    x = tk1.tokenize(text)\n",
    "    z = []\n",
    "    \n",
    "    for item in x:\n",
    "        y = tk2.tokenize(item)\n",
    "        total = len(y)\n",
    "        z.append(total)\n",
    "    \n",
    "    awl = round((sum(z)/k),0)\n",
    "    \n",
    "    return int(awl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5cb74417-2317-4470-b349-43da32102658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.txt\n",
      "Successful\n",
      "38.txt\n",
      "Successful\n",
      "39.txt\n",
      "Successful\n",
      "40.txt\n",
      "Successful\n",
      "41.txt\n",
      "Successful\n",
      "42.txt\n",
      "Successful\n",
      "43.txt\n",
      "Successful\n",
      "44.txt\n",
      "Successful\n",
      "45.txt\n",
      "Successful\n",
      "46.txt\n",
      "Successful\n",
      "47.txt\n",
      "Successful\n",
      "48.txt\n",
      "Successful\n",
      "49.txt\n",
      "Successful\n",
      "50.txt\n",
      "Successful\n",
      "51.txt\n",
      "Successful\n",
      "52.txt\n",
      "Successful\n",
      "53.txt\n",
      "Successful\n",
      "54.txt\n",
      "Successful\n",
      "55.txt\n",
      "Successful\n",
      "56.txt\n",
      "Successful\n",
      "57.txt\n",
      "Successful\n",
      "58.txt\n",
      "Successful\n",
      "59.txt\n",
      "Successful\n",
      "60.txt\n",
      "Successful\n",
      "61.txt\n",
      "Successful\n",
      "62.txt\n",
      "Successful\n",
      "63.txt\n",
      "Successful\n",
      "64.txt\n",
      "Successful\n",
      "65.txt\n",
      "Successful\n",
      "66.txt\n",
      "Successful\n",
      "67.txt\n",
      "Successful\n",
      "68.txt\n",
      "Successful\n",
      "69.txt\n",
      "Successful\n",
      "70.txt\n",
      "Successful\n",
      "71.txt\n",
      "Successful\n",
      "72.txt\n",
      "Successful\n",
      "73.txt\n",
      "Successful\n",
      "74.txt\n",
      "Successful\n",
      "75.txt\n",
      "Successful\n",
      "76.txt\n",
      "Successful\n",
      "77.txt\n",
      "Successful\n",
      "78.txt\n",
      "Successful\n",
      "79.txt\n",
      "Successful\n",
      "80.txt\n",
      "Successful\n",
      "81.txt\n",
      "Successful\n",
      "82.txt\n",
      "Successful\n",
      "83.txt\n",
      "Successful\n",
      "84.txt\n",
      "Successful\n",
      "85.txt\n",
      "Successful\n",
      "86.txt\n",
      "Successful\n",
      "87.txt\n",
      "Successful\n",
      "88.txt\n",
      "Successful\n",
      "89.txt\n",
      "Successful\n",
      "90.txt\n",
      "Successful\n",
      "91.txt\n",
      "Successful\n",
      "92.txt\n",
      "Successful\n",
      "93.txt\n",
      "Successful\n",
      "94.txt\n",
      "Successful\n",
      "95.txt\n",
      "Successful\n",
      "96.txt\n",
      "Successful\n",
      "97.txt\n",
      "Successful\n",
      "98.txt\n",
      "Successful\n",
      "99.txt\n",
      "Successful\n",
      "100.txt\n",
      "Successful\n",
      "101.txt\n",
      "Successful\n",
      "102.txt\n",
      "Successful\n",
      "103.txt\n",
      "Successful\n",
      "104.txt\n",
      "Successful\n",
      "105.txt\n",
      "Successful\n",
      "106.txt\n",
      "Successful\n",
      "107.txt\n",
      "Successful\n",
      "108.txt\n",
      "Successful\n",
      "109.txt\n",
      "Successful\n",
      "110.txt\n",
      "Successful\n",
      "111.txt\n",
      "Successful\n",
      "112.txt\n",
      "Successful\n",
      "113.txt\n",
      "Successful\n",
      "114.txt\n",
      "Successful\n",
      "115.txt\n",
      "Successful\n",
      "116.txt\n",
      "Successful\n",
      "117.txt\n",
      "Successful\n",
      "118.txt\n",
      "Successful\n",
      "119.txt\n",
      "Successful\n",
      "120.txt\n",
      "Successful\n",
      "121.txt\n",
      "Successful\n",
      "122.txt\n",
      "Successful\n",
      "123.txt\n",
      "Successful\n",
      "124.txt\n",
      "Successful\n",
      "125.txt\n",
      "Successful\n",
      "126.txt\n",
      "Successful\n",
      "127.txt\n",
      "Successful\n",
      "128.txt\n",
      "Successful\n",
      "129.txt\n",
      "Successful\n",
      "130.txt\n",
      "Successful\n",
      "131.txt\n",
      "Successful\n",
      "132.txt\n",
      "Successful\n",
      "133.txt\n",
      "Successful\n",
      "134.txt\n",
      "Successful\n",
      "135.txt\n",
      "Successful\n",
      "136.txt\n",
      "Successful\n",
      "137.txt\n",
      "Successful\n",
      "138.txt\n",
      "Successful\n",
      "139.txt\n",
      "Successful\n",
      "140.txt\n",
      "Successful\n",
      "141.txt\n",
      "Successful\n",
      "142.txt\n",
      "Successful\n",
      "143.txt\n",
      "Successful\n",
      "144.txt\n",
      "Successful\n",
      "145.txt\n",
      "Successful\n",
      "146.txt\n",
      "Successful\n",
      "147.txt\n",
      "Successful\n",
      "148.txt\n",
      "Successful\n",
      "149.txt\n",
      "Successful\n",
      "150.txt\n",
      "Successful\n",
      "CPU times: total: 312 ms\n",
      "Wall time: 349 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "awl = []\n",
    "\n",
    "\n",
    "for x in dir_list:\n",
    "    article = open((\"C:/Users/User/dataset/{}\".format(x)), encoding = 'mbcs').read().lower()\n",
    "    \n",
    "        \n",
    "    # Number of Words\n",
    "    k = count(article)\n",
    "    \n",
    "    \n",
    "    # Average Word Length\n",
    "    a = avg_word_length(article)\n",
    "    awl.append(a)\n",
    "    \n",
    "    \n",
    "    print(x)\n",
    "    print('Successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "095d415b-fc73-49a4-aed3-9739ecfdc701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(awl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d7b89-5ce6-44f7-9367-1b8873f03ec1",
   "metadata": {},
   "source": [
    "## Personal Pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffdec63-a582-458d-b67f-787b0435ddf7",
   "metadata": {},
   "source": [
    "To calculate Personal Pronouns mentioned in the text, we use regex to find the counts of the words - “I,” “we,” “my,” “ours,” and “us”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "be7643bc-b8f4-400f-9400-d8cfd2b4424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pronouns_count(text):\n",
    "    pk = RegexpTokenizer(\"((?:^I[\\s]|your|you|^he|^she|[\\s]its|[\\s]it[\\s]|^we|they|[\\s]them[\\s]|[\\s]+us+[\\s]|[\\s]him|[\\s]her|[\\s]his|theirs|[\\s]our|[\\s]my[\\s]))\")\n",
    "    \n",
    "    pp = pk.tokenize(text)\n",
    "    \n",
    "    return len(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0a65558d-a960-478d-95ce-9a27a5f52173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.txt\n",
      "Successful\n",
      "38.txt\n",
      "Successful\n",
      "39.txt\n",
      "Successful\n",
      "40.txt\n",
      "Successful\n",
      "41.txt\n",
      "Successful\n",
      "42.txt\n",
      "Successful\n",
      "43.txt\n",
      "Successful\n",
      "44.txt\n",
      "Successful\n",
      "45.txt\n",
      "Successful\n",
      "46.txt\n",
      "Successful\n",
      "47.txt\n",
      "Successful\n",
      "48.txt\n",
      "Successful\n",
      "49.txt\n",
      "Successful\n",
      "50.txt\n",
      "Successful\n",
      "51.txt\n",
      "Successful\n",
      "52.txt\n",
      "Successful\n",
      "53.txt\n",
      "Successful\n",
      "54.txt\n",
      "Successful\n",
      "55.txt\n",
      "Successful\n",
      "56.txt\n",
      "Successful\n",
      "57.txt\n",
      "Successful\n",
      "58.txt\n",
      "Successful\n",
      "59.txt\n",
      "Successful\n",
      "60.txt\n",
      "Successful\n",
      "61.txt\n",
      "Successful\n",
      "62.txt\n",
      "Successful\n",
      "63.txt\n",
      "Successful\n",
      "64.txt\n",
      "Successful\n",
      "65.txt\n",
      "Successful\n",
      "66.txt\n",
      "Successful\n",
      "67.txt\n",
      "Successful\n",
      "68.txt\n",
      "Successful\n",
      "69.txt\n",
      "Successful\n",
      "70.txt\n",
      "Successful\n",
      "71.txt\n",
      "Successful\n",
      "72.txt\n",
      "Successful\n",
      "73.txt\n",
      "Successful\n",
      "74.txt\n",
      "Successful\n",
      "75.txt\n",
      "Successful\n",
      "76.txt\n",
      "Successful\n",
      "77.txt\n",
      "Successful\n",
      "78.txt\n",
      "Successful\n",
      "79.txt\n",
      "Successful\n",
      "80.txt\n",
      "Successful\n",
      "81.txt\n",
      "Successful\n",
      "82.txt\n",
      "Successful\n",
      "83.txt\n",
      "Successful\n",
      "84.txt\n",
      "Successful\n",
      "85.txt\n",
      "Successful\n",
      "86.txt\n",
      "Successful\n",
      "87.txt\n",
      "Successful\n",
      "88.txt\n",
      "Successful\n",
      "89.txt\n",
      "Successful\n",
      "90.txt\n",
      "Successful\n",
      "91.txt\n",
      "Successful\n",
      "92.txt\n",
      "Successful\n",
      "93.txt\n",
      "Successful\n",
      "94.txt\n",
      "Successful\n",
      "95.txt\n",
      "Successful\n",
      "96.txt\n",
      "Successful\n",
      "97.txt\n",
      "Successful\n",
      "98.txt\n",
      "Successful\n",
      "99.txt\n",
      "Successful\n",
      "100.txt\n",
      "Successful\n",
      "101.txt\n",
      "Successful\n",
      "102.txt\n",
      "Successful\n",
      "103.txt\n",
      "Successful\n",
      "104.txt\n",
      "Successful\n",
      "105.txt\n",
      "Successful\n",
      "106.txt\n",
      "Successful\n",
      "107.txt\n",
      "Successful\n",
      "108.txt\n",
      "Successful\n",
      "109.txt\n",
      "Successful\n",
      "110.txt\n",
      "Successful\n",
      "111.txt\n",
      "Successful\n",
      "112.txt\n",
      "Successful\n",
      "113.txt\n",
      "Successful\n",
      "114.txt\n",
      "Successful\n",
      "115.txt\n",
      "Successful\n",
      "116.txt\n",
      "Successful\n",
      "117.txt\n",
      "Successful\n",
      "118.txt\n",
      "Successful\n",
      "119.txt\n",
      "Successful\n",
      "120.txt\n",
      "Successful\n",
      "121.txt\n",
      "Successful\n",
      "122.txt\n",
      "Successful\n",
      "123.txt\n",
      "Successful\n",
      "124.txt\n",
      "Successful\n",
      "125.txt\n",
      "Successful\n",
      "126.txt\n",
      "Successful\n",
      "127.txt\n",
      "Successful\n",
      "128.txt\n",
      "Successful\n",
      "129.txt\n",
      "Successful\n",
      "130.txt\n",
      "Successful\n",
      "131.txt\n",
      "Successful\n",
      "132.txt\n",
      "Successful\n",
      "133.txt\n",
      "Successful\n",
      "134.txt\n",
      "Successful\n",
      "135.txt\n",
      "Successful\n",
      "136.txt\n",
      "Successful\n",
      "137.txt\n",
      "Successful\n",
      "138.txt\n",
      "Successful\n",
      "139.txt\n",
      "Successful\n",
      "140.txt\n",
      "Successful\n",
      "141.txt\n",
      "Successful\n",
      "142.txt\n",
      "Successful\n",
      "143.txt\n",
      "Successful\n",
      "144.txt\n",
      "Successful\n",
      "145.txt\n",
      "Successful\n",
      "146.txt\n",
      "Successful\n",
      "147.txt\n",
      "Successful\n",
      "148.txt\n",
      "Successful\n",
      "149.txt\n",
      "Successful\n",
      "150.txt\n",
      "Successful\n",
      "CPU times: total: 203 ms\n",
      "Wall time: 208 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pro = []\n",
    "\n",
    "\n",
    "for x in dir_list:\n",
    "    article = open((\"C:/Users/User/dataset/{}\".format(x)), encoding = 'mbcs').read().lower()\n",
    "    \n",
    "        \n",
    "    # Average Word Length\n",
    "    p = pronouns_count(article)\n",
    "    pro.append(p)\n",
    "    \n",
    "    print(x)\n",
    "    print('Successful')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3105f371-22f9-4c60-8cc7-d6f831e653e3",
   "metadata": {},
   "source": [
    "print(pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d539840-8add-4e85-80a9-ffaf386c5576",
   "metadata": {},
   "source": [
    "## Syllable Count Per Word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaafe62-07e4-4433-b738-a5818943a9c6",
   "metadata": {},
   "source": [
    "We count the number of **Syllables** in each word of the text by counting the vowels present in each word. We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7fb68054-a32f-4c53-a237-1f0d58411f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(text):\n",
    "    sk = RegexpTokenizer(\"(?:[aeiouAEIOU][r|w]|[aA|eE|iI|oO|uU|yY][aeiouyAEIOUY]|[aeiouyAEIOUY]r|[a-zA-Z]y|es|ed|ear|[aeiouAEIOU])\")\n",
    "    \n",
    "    d = sk.tokenize(text)\n",
    "    sc = len(d)\n",
    "    \n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6f57b953-a8f6-4039-b8c4-fd9c901029dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count_per_word(text):\n",
    "    sk = RegexpTokenizer(\"(?:[aeiouAEIOU][r|w]|[aA|eE|iI|oO|uU|yY][aeiouyAEIOUY]|[aeiouyAEIOUY]r|[a-zA-Z]y|es|ed|ear|[aeiouAEIOU])\")\n",
    "    tk1 = RegexpTokenizer(\"[\\w']+\") # tokenize word\n",
    "    \n",
    "    d = sk.tokenize(text)\n",
    "    sc = len(d)\n",
    "    \n",
    "    r = tk1.tokenize(text)\n",
    "    wc = len(r)\n",
    "    \n",
    "    pw = sc/wc\n",
    "    \n",
    "    return pw    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5a79963b-ed5b-4f9d-9697-07a979f61730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.txt\n",
      "Successful\n",
      "38.txt\n",
      "Successful\n",
      "39.txt\n",
      "Successful\n",
      "40.txt\n",
      "Successful\n",
      "41.txt\n",
      "Successful\n",
      "42.txt\n",
      "Successful\n",
      "43.txt\n",
      "Successful\n",
      "44.txt\n",
      "Successful\n",
      "45.txt\n",
      "Successful\n",
      "46.txt\n",
      "Successful\n",
      "47.txt\n",
      "Successful\n",
      "48.txt\n",
      "Successful\n",
      "49.txt\n",
      "Successful\n",
      "50.txt\n",
      "Successful\n",
      "51.txt\n",
      "Successful\n",
      "52.txt\n",
      "Successful\n",
      "53.txt\n",
      "Successful\n",
      "54.txt\n",
      "Successful\n",
      "55.txt\n",
      "Successful\n",
      "56.txt\n",
      "Successful\n",
      "57.txt\n",
      "Successful\n",
      "58.txt\n",
      "Successful\n",
      "59.txt\n",
      "Successful\n",
      "60.txt\n",
      "Successful\n",
      "61.txt\n",
      "Successful\n",
      "62.txt\n",
      "Successful\n",
      "63.txt\n",
      "Successful\n",
      "64.txt\n",
      "Successful\n",
      "65.txt\n",
      "Successful\n",
      "66.txt\n",
      "Successful\n",
      "67.txt\n",
      "Successful\n",
      "68.txt\n",
      "Successful\n",
      "69.txt\n",
      "Successful\n",
      "70.txt\n",
      "Successful\n",
      "71.txt\n",
      "Successful\n",
      "72.txt\n",
      "Successful\n",
      "73.txt\n",
      "Successful\n",
      "74.txt\n",
      "Successful\n",
      "75.txt\n",
      "Successful\n",
      "76.txt\n",
      "Successful\n",
      "77.txt\n",
      "Successful\n",
      "78.txt\n",
      "Successful\n",
      "79.txt\n",
      "Successful\n",
      "80.txt\n",
      "Successful\n",
      "81.txt\n",
      "Successful\n",
      "82.txt\n",
      "Successful\n",
      "83.txt\n",
      "Successful\n",
      "84.txt\n",
      "Successful\n",
      "85.txt\n",
      "Successful\n",
      "86.txt\n",
      "Successful\n",
      "87.txt\n",
      "Successful\n",
      "88.txt\n",
      "Successful\n",
      "89.txt\n",
      "Successful\n",
      "90.txt\n",
      "Successful\n",
      "91.txt\n",
      "Successful\n",
      "92.txt\n",
      "Successful\n",
      "93.txt\n",
      "Successful\n",
      "94.txt\n",
      "Successful\n",
      "95.txt\n",
      "Successful\n",
      "96.txt\n",
      "Successful\n",
      "97.txt\n",
      "Successful\n",
      "98.txt\n",
      "Successful\n",
      "99.txt\n",
      "Successful\n",
      "100.txt\n",
      "Successful\n",
      "101.txt\n",
      "Successful\n",
      "102.txt\n",
      "Successful\n",
      "103.txt\n",
      "Successful\n",
      "104.txt\n",
      "Successful\n",
      "105.txt\n",
      "Successful\n",
      "106.txt\n",
      "Successful\n",
      "107.txt\n",
      "Successful\n",
      "108.txt\n",
      "Successful\n",
      "109.txt\n",
      "Successful\n",
      "110.txt\n",
      "Successful\n",
      "111.txt\n",
      "Successful\n",
      "112.txt\n",
      "Successful\n",
      "113.txt\n",
      "Successful\n",
      "114.txt\n",
      "Successful\n",
      "115.txt\n",
      "Successful\n",
      "116.txt\n",
      "Successful\n",
      "117.txt\n",
      "Successful\n",
      "118.txt\n",
      "Successful\n",
      "119.txt\n",
      "Successful\n",
      "120.txt\n",
      "Successful\n",
      "121.txt\n",
      "Successful\n",
      "122.txt\n",
      "Successful\n",
      "123.txt\n",
      "Successful\n",
      "124.txt\n",
      "Successful\n",
      "125.txt\n",
      "Successful\n",
      "126.txt\n",
      "Successful\n",
      "127.txt\n",
      "Successful\n",
      "128.txt\n",
      "Successful\n",
      "129.txt\n",
      "Successful\n",
      "130.txt\n",
      "Successful\n",
      "131.txt\n",
      "Successful\n",
      "132.txt\n",
      "Successful\n",
      "133.txt\n",
      "Successful\n",
      "134.txt\n",
      "Successful\n",
      "135.txt\n",
      "Successful\n",
      "136.txt\n",
      "Successful\n",
      "137.txt\n",
      "Successful\n",
      "138.txt\n",
      "Successful\n",
      "139.txt\n",
      "Successful\n",
      "140.txt\n",
      "Successful\n",
      "141.txt\n",
      "Successful\n",
      "142.txt\n",
      "Successful\n",
      "143.txt\n",
      "Successful\n",
      "144.txt\n",
      "Successful\n",
      "145.txt\n",
      "Successful\n",
      "146.txt\n",
      "Successful\n",
      "147.txt\n",
      "Successful\n",
      "148.txt\n",
      "Successful\n",
      "149.txt\n",
      "Successful\n",
      "150.txt\n",
      "Successful\n",
      "CPU times: total: 297 ms\n",
      "Wall time: 346 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "syl = []\n",
    "syl_w = []\n",
    "\n",
    "\n",
    "for x in dir_list:\n",
    "    article = open((\"C:/Users/User/dataset/{}\".format(x)), encoding = 'mbcs').read().lower()\n",
    "    \n",
    "        \n",
    "    # Syllable Count\n",
    "    h = syllable_count(article)\n",
    "    syl.append(h)\n",
    "    \n",
    "    \n",
    "    # Syllable Count Per Word\n",
    "    g = syllable_count_per_word(article)\n",
    "    syl_w.append(g)\n",
    "    \n",
    "    \n",
    "    print(x)\n",
    "    print('Successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "55a1b7b5-f5ee-431e-8b71-1718a696f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_excel('Output Data Structure.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "311f7321-0fe4-462c-be17-d4941d8ccdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE',\n",
       "       'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH',\n",
       "       'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',\n",
       "       'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT',\n",
       "       'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6a2fdc26-548d-49f3-8047-5ca8e3dc7749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save computed variable in output data structure\n",
    "\n",
    "output['POSITIVE SCORE'] = score_1\n",
    "output['NEGATIVE SCORE'] = score_2\n",
    "output['POLARITY SCORE'] = score_3\n",
    "output['SUBJECTIVITY SCORE'] = score_4\n",
    "\n",
    "output['AVG SENTENCE LENGTH'] = sent\n",
    "output['PERCENTAGE OF COMPLEX WORDS'] = percent\n",
    "output['FOG INDEX'] = fog_index\n",
    "\n",
    "output['AVG NUMBER OF WORDS PER SENTENCE'] = sent\n",
    "output['COMPLEX WORD COUNT'] = hard\n",
    "output['WORD COUNT'] = cl_w\n",
    "\n",
    "output['SYLLABLE PER WORD'] =  syl_w\n",
    "output['PERSONAL PRONOUNS'] = pro\n",
    "output['AVG WORD LENGTH'] = awl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a00d5bb4-7e6d-4f25-a0bc-2956611c2fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>67</td>\n",
       "      <td>36</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.10</td>\n",
       "      <td>23.95</td>\n",
       "      <td>0.22</td>\n",
       "      <td>9.67</td>\n",
       "      <td>23.95</td>\n",
       "      <td>401</td>\n",
       "      <td>1010</td>\n",
       "      <td>1.952794</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>60</td>\n",
       "      <td>35</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.16</td>\n",
       "      <td>18.64</td>\n",
       "      <td>0.13</td>\n",
       "      <td>7.51</td>\n",
       "      <td>18.64</td>\n",
       "      <td>188</td>\n",
       "      <td>590</td>\n",
       "      <td>1.676712</td>\n",
       "      <td>43</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>66</td>\n",
       "      <td>36</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.12</td>\n",
       "      <td>20.37</td>\n",
       "      <td>0.20</td>\n",
       "      <td>8.23</td>\n",
       "      <td>20.37</td>\n",
       "      <td>345</td>\n",
       "      <td>854</td>\n",
       "      <td>1.941109</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.12</td>\n",
       "      <td>19.03</td>\n",
       "      <td>0.12</td>\n",
       "      <td>7.66</td>\n",
       "      <td>19.03</td>\n",
       "      <td>221</td>\n",
       "      <td>793</td>\n",
       "      <td>1.611111</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>63</td>\n",
       "      <td>26</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.11</td>\n",
       "      <td>26.38</td>\n",
       "      <td>0.16</td>\n",
       "      <td>10.62</td>\n",
       "      <td>26.38</td>\n",
       "      <td>279</td>\n",
       "      <td>824</td>\n",
       "      <td>1.733406</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  URL_ID                                                URL  \\\n",
       "0           0      37  https://insights.blackcoffer.com/ai-in-healthc...   \n",
       "1           1      38  https://insights.blackcoffer.com/what-if-the-c...   \n",
       "2           2      39  https://insights.blackcoffer.com/what-jobs-wil...   \n",
       "3           3      40  https://insights.blackcoffer.com/will-machine-...   \n",
       "4           4      41  https://insights.blackcoffer.com/will-ai-repla...   \n",
       "\n",
       "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0              67              36            0.30                0.10   \n",
       "1              60              35            0.26                0.16   \n",
       "2              66              36            0.29                0.12   \n",
       "3              70              27            0.44                0.12   \n",
       "4              63              26            0.42                0.11   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0                23.95                         0.22       9.67   \n",
       "1                18.64                         0.13       7.51   \n",
       "2                20.37                         0.20       8.23   \n",
       "3                19.03                         0.12       7.66   \n",
       "4                26.38                         0.16      10.62   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                             23.95                 401        1010   \n",
       "1                             18.64                 188         590   \n",
       "2                             20.37                 345         854   \n",
       "3                             19.03                 221         793   \n",
       "4                             26.38                 279         824   \n",
       "\n",
       "   SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0           1.952794                 25                6  \n",
       "1           1.676712                 43                5  \n",
       "2           1.941109                 30                5  \n",
       "3           1.611111                 45                5  \n",
       "4           1.733406                 60                5  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d33a9aa3-9b17-4d3c-915b-56b29736805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_dict = {6.0 : \"Sixth grade\", 7.0: \"Seventh grade\", 8.0: \"Eighth grade\",\n",
    "                 9.0 : \"High school freshman\", 10.0 : \"High school sophomore\",\n",
    "                 11.0 : \"High school junior\", 12.0 : \"High school senior\",\n",
    "                 13.0 : \"College freshman\", 14.0 : \"College sophomore\",\n",
    "                 15.0 : \"College junior\" , 16.0 : \"College senior\",\n",
    "                 17.0 : \"College graduate\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8654614e-82fd-40c4-9cfa-7c2d88e0cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_level(num,grade):\n",
    "    for key,val in grade.items():\n",
    "            if key == round(num):\n",
    "                return val               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2a5b3fa6-ac31-402d-9603-da3f884f9626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 997 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rl = []\n",
    "\n",
    "for i in fog_index:\n",
    "    \n",
    "    #Reading level by grade\n",
    "    x = reading_level(i,rl_dict)\n",
    "    rl.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bc2e7126-cd02-4cdb-afa3-f14b3caa60af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new columns \"Total Syllable Count\" and \"Reading Level Grade\"\n",
    "\n",
    "output[\"TOTAL SYLLABLE COUNT\"] = syl\n",
    "output[\"READING LEVEL BY GRADE\"] = rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "af25f2a6-fd51-4a72-989e-c3d99373d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reordering the columns in pandas df\n",
    "\n",
    "output = output[[\"URL_ID\", \"URL\", \"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\",\n",
    "       \"SUBJECTIVITY SCORE\", \"AVG SENTENCE LENGTH\",\n",
    "       \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\", \"READING LEVEL BY GRADE\",\n",
    "       \"AVG NUMBER OF WORDS PER SENTENCE\", \"COMPLEX WORD COUNT\", \"WORD COUNT\",\n",
    "       \"SYLLABLE PER WORD\", \"TOTAL SYLLABLE COUNT\",\"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ef04fade-d4d9-4f7c-8657-ca511dd074f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>READING LEVEL BY GRADE</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>TOTAL SYLLABLE COUNT</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>67</td>\n",
       "      <td>36</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.10</td>\n",
       "      <td>23.95</td>\n",
       "      <td>0.22</td>\n",
       "      <td>9.67</td>\n",
       "      <td>High school sophomore</td>\n",
       "      <td>23.95</td>\n",
       "      <td>401</td>\n",
       "      <td>1010</td>\n",
       "      <td>1.952794</td>\n",
       "      <td>3599</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>60</td>\n",
       "      <td>35</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.16</td>\n",
       "      <td>18.64</td>\n",
       "      <td>0.13</td>\n",
       "      <td>7.51</td>\n",
       "      <td>Eighth grade</td>\n",
       "      <td>18.64</td>\n",
       "      <td>188</td>\n",
       "      <td>590</td>\n",
       "      <td>1.676712</td>\n",
       "      <td>2448</td>\n",
       "      <td>43</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>66</td>\n",
       "      <td>36</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.12</td>\n",
       "      <td>20.37</td>\n",
       "      <td>0.20</td>\n",
       "      <td>8.23</td>\n",
       "      <td>Eighth grade</td>\n",
       "      <td>20.37</td>\n",
       "      <td>345</td>\n",
       "      <td>854</td>\n",
       "      <td>1.941109</td>\n",
       "      <td>3362</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.12</td>\n",
       "      <td>19.03</td>\n",
       "      <td>0.12</td>\n",
       "      <td>7.66</td>\n",
       "      <td>Eighth grade</td>\n",
       "      <td>19.03</td>\n",
       "      <td>221</td>\n",
       "      <td>793</td>\n",
       "      <td>1.611111</td>\n",
       "      <td>2900</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>63</td>\n",
       "      <td>26</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.11</td>\n",
       "      <td>26.38</td>\n",
       "      <td>0.16</td>\n",
       "      <td>10.62</td>\n",
       "      <td>High school junior</td>\n",
       "      <td>26.38</td>\n",
       "      <td>279</td>\n",
       "      <td>824</td>\n",
       "      <td>1.733406</td>\n",
       "      <td>3160</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  POSITIVE SCORE  \\\n",
       "0      37  https://insights.blackcoffer.com/ai-in-healthc...              67   \n",
       "1      38  https://insights.blackcoffer.com/what-if-the-c...              60   \n",
       "2      39  https://insights.blackcoffer.com/what-jobs-wil...              66   \n",
       "3      40  https://insights.blackcoffer.com/will-machine-...              70   \n",
       "4      41  https://insights.blackcoffer.com/will-ai-repla...              63   \n",
       "\n",
       "   NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0              36            0.30                0.10                23.95   \n",
       "1              35            0.26                0.16                18.64   \n",
       "2              36            0.29                0.12                20.37   \n",
       "3              27            0.44                0.12                19.03   \n",
       "4              26            0.42                0.11                26.38   \n",
       "\n",
       "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX READING LEVEL BY GRADE  \\\n",
       "0                         0.22       9.67  High school sophomore   \n",
       "1                         0.13       7.51           Eighth grade   \n",
       "2                         0.20       8.23           Eighth grade   \n",
       "3                         0.12       7.66           Eighth grade   \n",
       "4                         0.16      10.62     High school junior   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                             23.95                 401        1010   \n",
       "1                             18.64                 188         590   \n",
       "2                             20.37                 345         854   \n",
       "3                             19.03                 221         793   \n",
       "4                             26.38                 279         824   \n",
       "\n",
       "   SYLLABLE PER WORD  TOTAL SYLLABLE COUNT  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0           1.952794                  3599                 25                6  \n",
       "1           1.676712                  2448                 43                5  \n",
       "2           1.941109                  3362                 30                5  \n",
       "3           1.611111                  2900                 45                5  \n",
       "4           1.733406                  3160                 60                5  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5544b293-95f9-431d-8a8d-d44d7c6f18d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame is written to Excel File successfully.\n"
     ]
    }
   ],
   "source": [
    "# determining the name of the file\n",
    "file_name = 'Output Data Structure.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "output.to_excel(file_name)\n",
    "print('DataFrame is written to Excel File successfully.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
